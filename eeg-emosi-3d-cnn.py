# -*- coding: utf-8 -*-
"""Copy_of_EEG_Emosi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dE6BvQvwSHrUPQOd0KHkTmMn8pSxwZ-i

#Skripsi - Rindu Tegar Senjawati

Identifikasi Emosi Melalui Sinyal EEG Menggunakan 3D CNN

#Library
"""

import tensorflow as tf
import csv
import pandas as pd
import numpy as np
import pywt
import os
import numpy as np
from scipy.signal import spectrogram
from scipy import signal
import scipy.signal as sps
from sklearn.preprocessing import minmax_scale
from sklearn.model_selection import train_test_split
from scipy.signal import get_window
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import normalize
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import minmax_scale
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

from numpy.lib.stride_tricks import as_strided

from tensorflow import keras
from keras.callbacks import Callback
from keras import backend as K
from keras.models import model_from_json
from keras.layers import BatchNormalization
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.utils import to_categorical
from keras.utils import to_categorical

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report

from sklearn.preprocessing import minmax_scale
from scipy.signal import get_window
import matplotlib.pyplot as plt
from scipy.signal import stft

import time

"""#Load Data"""

from google.colab import drive
drive.mount('/content/drive')

# newpath= "/content/drive/MyDrive/EEG-Emosi-3DCNN/Data"
newpath= "/content/drive/MyDrive/EEG-Emosi-3DCNN/Data-Baru"
Fkelas =["Negatif","Netral","Positif"]
numclasses=3

eegdatalatih=[]
def data_training():
    for kelas in Fkelas:
        pathkelas = os.path.join(newpath,kelas)
        numkelas = Fkelas.index(kelas)
        for edf in os.listdir(pathkelas):
                  edf_array = (pd.read_csv(os.path.join(pathkelas,edf)))
                  eegdatalatih.append([edf_array, numkelas])
data_training()

print(len(eegdatalatih))

"""#STFT"""

def make_frames(signal, frame_length, hop_length):
    num_frames = 1 + (len(signal) - frame_length) // hop_length
    frames = [signal[i * hop_length : i * hop_length + frame_length] for i in range(num_frames)]
    return np.array(frames)

def make_window(length):
    return np.hamming(length)

def stft(signal, frame_length, hop_length, sample_rate, min_freq, max_freq):
    frames = make_frames(signal, frame_length, hop_length)
    window = make_window(frame_length)
    stft_result = np.fft.fft(frames * window, axis=1)

    # Calculate frequency and time axes
    freq_axis = np.fft.fftfreq(frame_length, 1 / sample_rate)
    time_axis = np.arange(stft_result.shape[0]) * hop_length / sample_rate

    # Crop STFT result to show only the frequency range 4-45 Hz
    freq_mask = (freq_axis >= min_freq) & (freq_axis <= max_freq)
    stft_result = np.abs(stft_result[:, freq_mask])
    freq_axis = freq_axis[freq_mask]

    return stft_result, freq_axis, time_axis

import numpy as np
from sklearn.preprocessing import minmax_scale
from scipy.signal import get_window
import matplotlib.pyplot as plt

FT7, FT8, T7, C5, C6, T8, TP7, CP5, CP6, TP8, P7, P8 = [], [], [], [], [], [], [], [], [], [], [], []
datalatih = []
newdata = []
vektor_input = []
vektor_input1 = []

def filter():
    for i in range(len(eegdatalatih)):
        FT7 = eegdatalatih[i][0][eegdatalatih[i][0].columns[0:1]].values
        FT8 = eegdatalatih[i][0][eegdatalatih[i][0].columns[1:2]].values
        T7 = eegdatalatih[i][0][eegdatalatih[i][0].columns[2:3]].values
        C5 = eegdatalatih[i][0][eegdatalatih[i][0].columns[3:4]].values
        C6 = eegdatalatih[i][0][eegdatalatih[i][0].columns[4:5]].values
        T8 = eegdatalatih[i][0][eegdatalatih[i][0].columns[5:6]].values
        TP7 = eegdatalatih[i][0][eegdatalatih[i][0].columns[6:7]].values
        CP5 = eegdatalatih[i][0][eegdatalatih[i][0].columns[7:8]].values
        CP6 = eegdatalatih[i][0][eegdatalatih[i][0].columns[8:9]].values
        TP8 = eegdatalatih[i][0][eegdatalatih[i][0].columns[9:10]].values
        P7 = eegdatalatih[i][0][eegdatalatih[i][0].columns[10:11]].values
        P8 = eegdatalatih[i][0][eegdatalatih[i][0].columns[11:12]].values

        FT7 = np.reshape(FT7, 1000)
        FT8 = np.reshape(FT8, 1000)
        T7 = np.reshape(T7, 1000)
        C5 = np.reshape(C5, 1000)
        C6 = np.reshape(C6, 1000)
        T8 = np.reshape(T8, 1000)
        TP7 = np.reshape(TP7, 1000)
        CP5 = np.reshape(CP5, 1000)
        CP6 = np.reshape(CP6, 1000)
        TP8 = np.reshape(TP8, 1000)
        P7 = np.reshape(P7, 1000)
        P8 = np.reshape(P8, 1000)

        newdata = [FT7] + [FT8] + [T7] + [C5] + [C6] + [T8] + [TP7] + [CP5] + [CP6] + [TP8] + [P7] + [P8]
        gelombang = []
        vektor_input1.append([np.concatenate((FT7, FT8, T7, C5, C6, T8, TP7, CP5, CP6, TP8, P7, P8), axis=None), eegdatalatih[i][1]])
        for j in range(12):
            data = newdata[j]
            frames, freq_axis, time_axis = stft(data, 200, 100, 200, 4, 45)
            gelombang.append([np.concatenate((frames), axis=None)])
        # print(frames.shape)
        # print(gelombang[0][0].shape)
        vektor_gelombang = np.concatenate((gelombang[0], gelombang[1], gelombang[2], gelombang[3],
                                           gelombang[4], gelombang[5], gelombang[6], gelombang[7],
                                           gelombang[8], gelombang[9], gelombang[10], gelombang[11]
                                           ), axis=None)

        vektor_input.append([vektor_gelombang, eegdatalatih[i][1]])
filter()

print(vektor_input[0][0].shape)

data_baru = []
for i in range(len(vektor_input)):
  temp = vektor_input[i][0]
  data = np.array(temp).reshape((12, 42, 9))
  data_baru.append([data, vektor_input[i][1]])
print(data_baru[0][0].shape)

# 1d
data_baru1 = []
for i in range(len(vektor_input)):
  temp1 = vektor_input[i][0]
  data1 = np.array(temp1).reshape((4536))
  data_baru1.append([data1, vektor_input[i][1]])
print(data_baru1[0][0].shape)

# 2d
data_baru2 = []
for i in range(len(vektor_input)):
  temp2 = vektor_input[i][0]
  data2 = np.array(temp2).reshape((12, 378))
  data_baru2.append([data2, vektor_input[i][1]])
print(data_baru2[0][0].shape)

"""#Split Data"""

#split data latih dan uji
x_train,x_test,y_train,y_test = [], [], [], []

X = [] #fitur
Y = [] #label kelas

for features,label in vektor_input:
    X.append(features)
    Y.append(label)
print(len(X[0]))
X = np.array(X)

#Split X,Y to train and test
x_train,x_test,y_train,y_test = train_test_split(X, Y, train_size=0.8)
x_train,x_val,y_train,y_val = train_test_split(x_train, y_train, train_size=0.8)

print("Ukuran x_train :",x_train.shape)
print("Ukuran x_val   :",x_val.shape)
print("Ukuran x_test  :",x_test.shape)

print("Ukuran y_train :",len(y_train))
print("Ukuran y_val   :",len(y_val))
print("Ukuran y_test  :",len(y_test))

y_train = to_categorical(y_train, numclasses)
y_val = to_categorical(y_val,  numclasses)
y_test = to_categorical(y_test,  numclasses)

print("Ukuran y_train :",y_train.shape)
print("Ukuran y_val   :",y_val.shape)
print("Ukuran y_test  :",y_test.shape)

print("Contoh kelas : ")
for i in range (len(x_train)):
    print(i)
    print(x_train[i])

import numpy as np
from keras.utils import to_categorical

# Misalnya, Anda memiliki 3 kelas (0, 1, 2) dalam label target
num_classes = 3

# Pastikan y_train dan y_test dalam bentuk array NumPy dengan bentuk (num_samples,)
y_train = np.array(y_train)
y_test = np.array(y_test)

# Mengonversi y_train dan y_test ke format one-hot
y_train_one_hot = to_categorical(y_train, num_classes)
y_test_one_hot = to_categorical(y_test, num_classes)

# Pastikan bahwa bentuk keluaran sesuai dengan bentuk label target yang diharapkan
print(y_train_one_hot.shape)  # Misalnya, ini harus menghasilkan (num_samples, num_classes)
print(y_test_one_hot.shape)   # Misalnya, ini juga harus menghasilkan (num_samples, num_classes)

"""#CNN - 1D, 2D

1D CNN
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout

# Definisikan ukuran input
panjang_input = 4536
jumlah_kanal = 1

model1d = Sequential()
model1d.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(panjang_input, jumlah_kanal)))
model1d.add(MaxPooling1D(pool_size=2, padding='same'))
model1d.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model1d.add(MaxPooling1D(pool_size=2, padding='same'))
model1d.add(Flatten())
model1d.add(Dense(units=128, activation='relu'))
model1d.add(Dropout(0.5))
model1d.add(Dense(units=3, activation='softmax'))

# Tampilkan ringkasan model
model1d.summary()

"""2D CNN"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Definisikan ukuran input
tinggi_input = 12
lebar_input = 378
jumlah_kanal = 1

model2d = Sequential()
model2d.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(tinggi_input, lebar_input, jumlah_kanal)))
model2d.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model2d.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))
model2d.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model2d.add(Flatten())
model2d.add(Dense(units=128, activation='relu'))
model2d.add(Dropout(0.5))
model2d.add(Dense(units=3, activation='softmax'))

# Tampilkan ringkasan model
model2d.summary()

model1d.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['categorical_accuracy'])
model2d.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['categorical_accuracy'])

"""Learning"""

# Reshape data untuk model1d
x_train_reshaped = x_train.reshape((len(x_train), panjang_input, jumlah_kanal))
x_val_reshaped = x_val.reshape((len(x_val), panjang_input, jumlah_kanal))
x_test_reshaped = x_test.reshape((len(x_test), panjang_input, jumlah_kanal))

# Training model1d
print("Training model dimulai")
start_time = time.time()
history_model1d = model1d.fit(x_train_reshaped, y_train, epochs=200, validation_data=(x_val_reshaped, y_val), batch_size=32, shuffle=True)
print("Training model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60), "menit")

# Reshape data untuk model2d
x_train_reshaped_2d = x_train.reshape((len(x_train), tinggi_input, lebar_input, jumlah_kanal))
x_val_reshaped_2d = x_val.reshape((len(x_val), tinggi_input, lebar_input, jumlah_kanal))
x_test_reshaped_2d = x_test.reshape((len(x_test), tinggi_input, lebar_input, jumlah_kanal))

print("Training model dimulai")
start_time = time.time()
# Training model2d
history_model2d = model2d.fit(x_train_reshaped_2d, y_train, epochs=200, validation_data=(x_val_reshaped_2d, y_val), batch_size=32, shuffle=True)
print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

# Evaluasi model1d
score_model1d = model1d.evaluate(x_test_reshaped, y_test)
print('Test Loss Model1D: ', score_model1d[0])
print('Test Accuracy Model1D: ', score_model1d[1] * 100.0)

# Evaluasi model2d
score_model2d = model2d.evaluate(x_test_reshaped_2d, y_test)
print('Test Loss Model2D: ', score_model2d[0])
print('Test Accuracy Model2D: ', score_model2d[1] * 100.0)

"""#3D CNN

#3D CNN - SGD

cnn 4 layer - SGD - lr: 0.0001
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import SGD
import time

kanal = 12
frekuensi = 42
waktu = 9

model4SGD1 = Sequential()

model4SGD1.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model4SGD1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model4SGD1.add(Flatten())
model4SGD1.add(Dense(units=3, activation='softmax'))

model4SGD1.summary()

custom_optimizer = SGD(learning_rate=0.0001)
model4SGD1.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history4SGD1 = model4SGD1.fit(x_train, y_train,
                            epochs=200,
                            validation_data=(x_val, y_val),
                            batch_size=32,
                            shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 4 layer - SDG - lr:0.0010"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import SGD
import time

kanal = 12
frekuensi = 42
waktu = 9

model4SGD2 = Sequential()

model4SGD2.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model4SGD2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model4SGD2.add(Flatten())
model4SGD2.add(Dense(units=3, activation='softmax'))

custom_optimizer = SGD(learning_rate=0.0010)

model4SGD2.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history4SGD2 = model4SGD2.fit(x_train, y_train,
                            epochs=200,
                            validation_data=(x_val, y_val),
                            batch_size=32,
                            shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 4 layer - SGD - lr:0.0100"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import SGD
import time

kanal = 12
frekuensi = 42
waktu = 9

model4SGD3 = Sequential()

model4SGD3.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model4SGD3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model4SGD3.add(Flatten())
model4SGD3.add(Dense(units=3, activation='softmax'))

model4SGD3.summary()

custom_optimizer = SGD(learning_rate=0.0100)

model4SGD3.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history4SGD3 = model4SGD3.fit(x_train, y_train,
                            epochs=200,
                            validation_data=(x_val, y_val),
                            batch_size=32,
                            shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 8 layer - SGD - lr:0.0001"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import SGD
import time

kanal = 12
frekuensi = 42
waktu = 9

# Ubah ukuran pooling menjadi (3, 3, 3) untuk mengurangi laju downsampling
model8SGD1 = Sequential()
model8SGD1.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model8SGD1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8SGD1.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu'))
model8SGD1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8SGD1.add(Flatten())
model8SGD1.add(Dense(units=16, activation='relu'))
model8SGD1.add(Dropout(0.5))
model8SGD1.add(Dense(units=3, activation='softmax'))

custom_optimizer = SGD(learning_rate=0.0001)

model8SGD1.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

# Reshape data
x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history8SGD1 = model8SGD1.fit(x_train, y_train,
                                        epochs=200,
                                        validation_data=(x_val, y_val),
                                        batch_size=32,
                                        shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 8 layer - SGD - lr:0.0010"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import SGD
import time

kanal = 12
frekuensi = 42
waktu = 9

model8SGD2 = Sequential()

model8SGD2.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model8SGD2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8SGD2.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu'))
model8SGD2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8SGD2.add(Flatten())
model8SGD2.add(Dense(units=16, activation='relu'))
model8SGD2.add(Dropout(0.5))
model8SGD2.add(Dense(units=3, activation='softmax'))

model8SGD2.summary()

custom_optimizer = SGD(learning_rate=0.0010)
model8SGD2.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

# Reshape data
x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history8SGD2 = model8SGD2.fit(x_train, y_train,
                              epochs=200,
                              validation_data=(x_val, y_val),
                              batch_size=32,
                              shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 8 layer - SGD - lr:0.0100"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import SGD
import time

kanal = 12
frekuensi = 42
waktu = 9

model8SGD3 = Sequential()

model8SGD3.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model8SGD3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8SGD3.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu'))
model8SGD3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8SGD3.add(Flatten())
model8SGD3.add(Dense(units=16, activation='relu'))
model8SGD3.add(Dropout(0.5))
model8SGD3.add(Dense(units=3, activation='softmax'))

model8SGD3.summary()

custom_optimizer = SGD(learning_rate=0.0100)
model8SGD3.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

# Reshape data
x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history8SGD3 = model8SGD3.fit(x_train, y_train,
                              epochs=200,
                              validation_data=(x_val, y_val),
                              batch_size=32,
                              shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 16 layer - SGD - lr:0.0001"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import SGD
import time

kanal = 12
frekuensi = 42
waktu = 9

model16SGD1 = Sequential()

model16SGD1.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1), padding='same'))
model16SGD1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16SGD1.add(Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16SGD1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16SGD1.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16SGD1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16SGD1.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16SGD1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16SGD1.add(Flatten())
model16SGD1.add(Dense(units=128, activation='relu'))
model16SGD1.add(Dropout(0.5))
model16SGD1.add(Dense(units=64, activation='relu'))
model16SGD1.add(Dropout(0.5))
model16SGD1.add(Dense(units=32, activation='relu'))
model16SGD1.add(Dropout(0.5))
model16SGD1.add(Dense(units=3, activation='softmax'))

custom_optimizer = SGD(learning_rate=0.0001)

model16SGD1.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history16SGD1 = model16SGD1.fit(x_train, y_train,
                                epochs=200,
                                validation_data=(x_val, y_val),
                                batch_size=32,
                                shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 16 layer - SGD - lr:0.0010"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import SGD
import time

kanal = 12
frekuensi = 42
waktu = 9

model16SGD2 = Sequential()

model16SGD2.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1), padding='same'))
model16SGD2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16SGD2.add(Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16SGD2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16SGD2.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16SGD2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16SGD2.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16SGD2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16SGD2.add(Flatten())
model16SGD2.add(Dense(units=128, activation='relu'))
model16SGD2.add(Dropout(0.5))
model16SGD2.add(Dense(units=64, activation='relu'))
model16SGD2.add(Dropout(0.5))
model16SGD2.add(Dense(units=32, activation='relu'))
model16SGD2.add(Dropout(0.5))
model16SGD2.add(Dense(units=3, activation='softmax'))

custom_optimizer = SGD(learning_rate=0.0010)

model16SGD2.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history16SGD2 = model16SGD2.fit(x_train, y_train,
                                epochs=200,
                                validation_data=(x_val, y_val),
                                batch_size=32,
                                shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 16 layer - SGD - lr:0.0100"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import SGD
import time

kanal = 12
frekuensi = 42
waktu = 9

model16SGD3 = Sequential()

model16SGD3.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1), padding='same'))
model16SGD3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16SGD3.add(Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16SGD3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16SGD3.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16SGD3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16SGD3.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16SGD3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16SGD3.add(Flatten())
model16SGD3.add(Dense(units=128, activation='relu'))
model16SGD3.add(Dropout(0.5))
model16SGD3.add(Dense(units=64, activation='relu'))
model16SGD3.add(Dropout(0.5))
model16SGD3.add(Dense(units=32, activation='relu'))
model16SGD3.add(Dropout(0.5))
model16SGD3.add(Dense(units=3, activation='softmax'))

custom_optimizer = SGD(learning_rate=0.0100)

model16SGD3.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history16SGD3 = model16SGD3.fit(x_train, y_train,
                                epochs=200,
                                validation_data=(x_val, y_val),
                                batch_size=32,
                                shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""Akurasi"""

#Calculate loss and accuracy
# score4SGD1 = model4SGD1.evaluate(x_test,y_test)
# score4SGD2 = model4SGD2.evaluate(x_test,y_test)
score4SGD3 = model4SGD3.evaluate(x_test,y_test)
# score8SGD1 = model8SGD1.evaluate(x_test,y_test)
# score8SGD2 = model8SGD2.evaluate(x_test,y_test)
score8SGD3 = model8SGD3.evaluate(x_test,y_test)
# score16SGD1 = model16SGD1.evaluate(x_test,y_test)
# score16SGD2 = model16SGD2.evaluate(x_test,y_test)
score16SGD3 = model16SGD3.evaluate(x_test,y_test)

# print('Test Loss SGD 4 layer lr = 0.0001 : ',score4SGD1[0])
# print('Test Accuracy SGD 4 layer lr = 0.0001: ',score4SGD1[1]*100.0)
# print('========================================')
# print('Test Loss SGD 4 layer lr = 0.0010 : ',score4SGD2[0])
# print('Test Accuracy SGD 4 layer lr = 0.0010: ',score4SGD2[1]*100.0)
# print('========================================')
print('Test Loss SGD 4 layer lr = 0.0100 : ',score4SGD3[0])
print('Test Accuracy SGD 4 layer lr = 0.0100: ',score4SGD3[1]*100.0)
print('========================================')

# print('Test Loss SGD 8 layer lr = 0.0001 : ',score8SGD1[0])
# print('Test Accuracy SGD 8 layer lr = 0.0001: ',score8SGD1[1]*100.0)
# print('========================================')
# print('Test Loss SGD 8 layer lr = 0.0010 : ',score8SGD2[0])
# print('Test Accuracy SGD 8 layer lr = 0.0010: ',score8SGD2[1]*100.0)
# print('========================================')
print('Test Loss SGD 8 layer lr = 0.0100 : ',score8SGD3[0])
print('Test Accuracy SGD 8 layer lr = 0.0100: ',score8SGD3[1]*100.0)
print('========================================')

# print('Test Loss SGD 16 layer lr = 0.0001 : ',score16SGD1[0])
# print('Test Accuracy SGD 16 layer lr = 0.0001: ',score16SGD1[1]*100.0)
# print('========================================')
# print('Test Loss SGD 16 layer lr = 0.0010 : ',score16SGD2[0])
# print('Test Accuracy SGD 16 layer lr = 0.0010: ',score16SGD2[1]*100.0)
# print('========================================')
print('Test Loss SGD 16 layer lr = 0.0100 : ',score16SGD3[0])
print('Test Accuracy SGD 16 layer lr = 0.0100: ',score16SGD3[1]*100.0)
print('========================================')

"""#3D CNN - ADAM

cnn 4 layer - Adam - lr:0.0001
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import time

kanal = 12
frekuensi = 42
waktu = 9

model4Adam1 = Sequential()

model4Adam1.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model4Adam1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model4Adam1.add(Flatten())
model4Adam1.add(Dense(units=3, activation='softmax'))

model4Adam1.summary()

custom_optimizer = Adam(learning_rate=0.0001)
model4Adam1.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history4Adam1 = model4Adam1.fit(x_train, y_train,
                            epochs=200,
                            validation_data=(x_val, y_val),
                            batch_size=32,
                            shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 4 layer - Adam - lr:0.0010"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import time

kanal = 12
frekuensi = 42
waktu = 9

model4Adam2 = Sequential()

model4Adam2.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model4Adam2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model4Adam2.add(Flatten())
model4Adam2.add(Dense(units=3, activation='softmax'))

model4Adam2.summary()

custom_optimizer = Adam(learning_rate=0.0010)
model4Adam2.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history4Adam2 = model4Adam2.fit(x_train, y_train,
                                epochs=200,
                                validation_data=(x_val, y_val),
                                batch_size=32,
                                shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 4 layer - Adam - lr: 0.0100"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import time

kanal = 12
frekuensi = 42
waktu = 9

model4Adam3 = Sequential()

model4Adam3.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model4Adam3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model4Adam3.add(Flatten())
model4Adam3.add(Dense(units=3, activation='softmax'))

model4Adam3.summary()

custom_optimizer = Adam(learning_rate=0.0100)
model4Adam3.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history4Adam3 = model4Adam3.fit(x_train, y_train,
                                epochs=200,
                                validation_data=(x_val, y_val),
                                batch_size=32,
                                shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 8 layer - Adam - r:0.0001"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import time

kanal = 12
frekuensi = 42
waktu = 9

# Ubah ukuran pooling menjadi (3, 3, 3) untuk mengurangi laju downsampling
model8Adam1 = Sequential()
model8Adam1.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model8Adam1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8Adam1.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu'))
model8Adam1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8Adam1.add(Flatten())
model8Adam1.add(Dense(units=16, activation='relu'))
model8Adam1.add(Dropout(0.5))
model8Adam1.add(Dense(units=3, activation='softmax'))

custom_optimizer = Adam(learning_rate=0.0001)

model8Adam1.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

# Reshape data
x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history8Adam1 = model8Adam1.fit(x_train, y_train,
                                epochs=200,
                                validation_data=(x_val, y_val),
                                batch_size=32,
                                shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 8 layer - Adam - r:0.0010"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import time

kanal = 12
frekuensi = 42
waktu = 9

# Ubah ukuran pooling menjadi (3, 3, 3) untuk mengurangi laju downsampling
model8Adam2 = Sequential()
model8Adam2.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model8Adam2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8Adam2.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu'))
model8Adam2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8Adam2.add(Flatten())
model8Adam2.add(Dense(units=16, activation='relu'))
model8Adam2.add(Dropout(0.5))
model8Adam2.add(Dense(units=3, activation='softmax'))

custom_optimizer = Adam(learning_rate=0.0010)

model8Adam2.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

# Reshape data
x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history8Adam2 = model8Adam2.fit(x_train, y_train,
                                epochs=200,
                                validation_data=(x_val, y_val),
                                batch_size=32,
                                shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 8 layer - Adam - r:0.0100"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import time

kanal = 12
frekuensi = 42
waktu = 9

# Ubah ukuran pooling menjadi (3, 3, 3) untuk mengurangi laju downsampling
model8Adam3 = Sequential()
model8Adam3.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model8Adam3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8Adam3.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu'))
model8Adam3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8Adam3.add(Flatten())
model8Adam3.add(Dense(units=16, activation='relu'))
model8Adam3.add(Dropout(0.5))
model8Adam3.add(Dense(units=3, activation='softmax'))

custom_optimizer = Adam(learning_rate=0.0100)

model8Adam3.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

# Reshape data
x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history8Adam3 = model8Adam3.fit(x_train, y_train,
                                epochs=200,
                                validation_data=(x_val, y_val),
                                batch_size=32,
                                shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 16 layer - Adam - r:0.0001"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import time

kanal = 12
frekuensi = 42
waktu = 9

model16Adam1 = Sequential()

model16Adam1.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1), padding='same'))
model16Adam1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adam1.add(Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adam1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adam1.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adam1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adam1.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adam1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adam1.add(Flatten())
model16Adam1.add(Dense(units=128, activation='relu'))
model16Adam1.add(Dropout(0.5))
model16Adam1.add(Dense(units=64, activation='relu'))
model16Adam1.add(Dropout(0.5))
model16Adam1.add(Dense(units=32, activation='relu'))
model16Adam1.add(Dropout(0.5))
model16Adam1.add(Dense(units=3, activation='softmax'))

custom_optimizer = Adam(learning_rate=0.0001)

model16Adam1.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history16Adam1 = model16Adam1.fit(x_train, y_train,
                                  epochs=200,
                                  validation_data=(x_val, y_val),
                                  batch_size=32,
                                  shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 16 layer - Adam - r:0.0010"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import time

kanal = 12
frekuensi = 42
waktu = 9

model16Adam2 = Sequential()

model16Adam2.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1), padding='same'))
model16Adam2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adam2.add(Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adam2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adam2.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adam2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adam2.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adam2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adam2.add(Flatten())
model16Adam2.add(Dense(units=128, activation='relu'))
model16Adam2.add(Dropout(0.5))
model16Adam2.add(Dense(units=64, activation='relu'))
model16Adam2.add(Dropout(0.5))
model16Adam2.add(Dense(units=32, activation='relu'))
model16Adam2.add(Dropout(0.5))
model16Adam2.add(Dense(units=3, activation='softmax'))

custom_optimizer = Adam(learning_rate=0.0010)

model16Adam2.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history16Adam2 = model16Adam2.fit(x_train, y_train,
                                  epochs=200,
                                  validation_data=(x_val, y_val),
                                  batch_size=32,
                                  shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 16 layer - Adam - r:0.0100"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import time

kanal = 12
frekuensi = 42
waktu = 9

model16Adam3 = Sequential()

model16Adam3.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1), padding='same'))
model16Adam3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adam3.add(Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adam3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adam3.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adam3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adam3.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adam3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adam3.add(Flatten())
model16Adam3.add(Dense(units=128, activation='relu'))
model16Adam3.add(Dropout(0.5))
model16Adam3.add(Dense(units=64, activation='relu'))
model16Adam3.add(Dropout(0.5))
model16Adam3.add(Dense(units=32, activation='relu'))
model16Adam3.add(Dropout(0.5))
model16Adam3.add(Dense(units=3, activation='softmax'))

custom_optimizer = Adam(learning_rate=0.0100)

model16Adam3.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history16Adam3 = model16Adam3.fit(x_train, y_train,
                                  epochs=200,
                                  validation_data=(x_val, y_val),
                                  batch_size=32,
                                  shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""Akurasi"""

#Calculate loss and accuracy
# score4Adam1 = model4Adam1.evaluate(x_test,y_test)
# score4Adam2 = model4Adam2.evaluate(x_test,y_test)
score4Adam3 = model4Adam3.evaluate(x_test,y_test)
# score8Adam1 = model8Adam1.evaluate(x_test,y_test)
# score8Adam2 = model8Adam2.evaluate(x_test,y_test)
score8Adam3 = model8Adam3.evaluate(x_test,y_test)
# score16Adam1 = model16Adam1.evaluate(x_test,y_test)
# score16Adam2 = model16Adam2.evaluate(x_test,y_test)
score16Adam3 = model16Adam3.evaluate(x_test,y_test)

# print('Test Loss Adam 4 layer lr = 0.0001 : ',score4Adam1[0])
# print('Test Accuracy Adam 4 layer lr = 0.0001: ',score4Adam1[1]*100.0)
# print('========================================')
# print('Test Loss Adam 4 layer lr = 0.0010 : ',score4Adam2[0])
# print('Test Accuracy Adam 4 layer lr = 0.0010: ',score4Adam2[1]*100.0)
# print('========================================')
print('Test Loss Adam 4 layer lr = 0.0100 : ',score4Adam3[0])
print('Test Accuracy Adam 4 layer lr = 0.0100: ',score4Adam3[1]*100.0)
print('========================================')

# print('Test Loss Adam 8 layer lr = 0.0001 : ',score8Adam1[0])
# print('Test Accuracy Adam 8 layer lr = 0.0001: ',score8Adam1[1]*100.0)
# print('========================================')
# print('Test Loss Adam 8 layer lr = 0.0010 : ',score8Adam2[0])
# print('Test Accuracy Adam 8 layer lr = 0.0010: ',score8Adam2[1]*100.0)
# print('========================================')
print('Test Loss Adam 8 layer lr = 0.0100 : ',score8Adam3[0])
print('Test Accuracy Adam 8 layer lr = 0.0100: ',score8Adam3[1]*100.0)
print('========================================')

# print('Test Loss Adam 16 layer lr = 0.0001 : ',score16Adam1[0])
# print('Test Accuracy Adam 16 layer lr = 0.0001: ',score16Adam1[1]*100.0)
# print('========================================')
# print('Test Loss Adam 16 layer lr = 0.0010 : ',score16Adam2[0])
# print('Test Accuracy Adam 16 layer lr = 0.0010: ',score16Adam2[1]*100.0)
# print('========================================')
print('Test Loss Adam 16 layer lr = 0.0100 : ',score16Adam3[0])
print('Test Accuracy Adam 16 layer lr = 0.0100: ',score16Adam3[1]*100.0)
print('========================================')

"""#3D CNN - ADAMAX

cnn 4 layer - Adamax - r:0.0001
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adamax
import time

kanal = 12
frekuensi = 42
waktu = 9

model4Adamax1 = Sequential()

model4Adamax1.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model4Adamax1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model4Adamax1.add(Flatten())
model4Adamax1.add(Dense(units=3, activation='softmax'))

model4Adamax1.summary()

custom_optimizer = Adamax(learning_rate=0.0001)
model4Adamax1.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history4Adamax1 = model4Adamax1.fit(x_train, y_train,
                                    epochs=200,
                                    validation_data=(x_val, y_val),
                                    batch_size=32,
                                    shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 4 layer - Adamax - r:0.0010"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adamax
import time

kanal = 12
frekuensi = 42
waktu = 9

model4Adamax2 = Sequential()

model4Adamax2.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model4Adamax2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model4Adamax2.add(Flatten())
model4Adamax2.add(Dense(units=3, activation='softmax'))

model4Adamax2.summary()

custom_optimizer = Adamax(learning_rate=0.0010)
model4Adamax2.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history4Adamax2 = model4Adamax2.fit(x_train, y_train,
                                    epochs=200,
                                    validation_data=(x_val, y_val),
                                    batch_size=32,
                                    shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 4 layer - Adamax - r:0.0100"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adamax
import time

kanal = 12
frekuensi = 42
waktu = 9

model4Adamax3 = Sequential()

model4Adamax3.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model4Adamax3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model4Adamax3.add(Flatten())
model4Adamax3.add(Dense(units=3, activation='softmax'))

model4Adamax3.summary()

custom_optimizer = Adamax(learning_rate=0.0100)
model4Adamax3.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history4Adamax3 = model4Adamax3.fit(x_train, y_train,
                                    epochs=200,
                                    validation_data=(x_val, y_val),
                                    batch_size=32,
                                    shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 8 layer - Adamax - r:0.0001"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adamax
import time

kanal = 12
frekuensi = 42
waktu = 9

# Ubah ukuran pooling menjadi (3, 3, 3) untuk mengurangi laju downsampling
model8Adamax1 = Sequential()
model8Adamax1.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model8Adamax1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8Adamax1.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu'))
model8Adamax1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8Adamax1.add(Flatten())
model8Adamax1.add(Dense(units=16, activation='relu'))
model8Adamax1.add(Dropout(0.5))
model8Adamax1.add(Dense(units=3, activation='softmax'))

custom_optimizer = Adamax(learning_rate=0.0001)

model8Adamax1.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

# Reshape data
x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history8Adamax1 = model8Adamax1.fit(x_train, y_train,
                                     epochs=200,
                                     validation_data=(x_val, y_val),
                                     batch_size=32,
                                     shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 8 layer - Adamax - r:0.0010"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adamax
import time

kanal = 12
frekuensi = 42
waktu = 9

# Ubah ukuran pooling menjadi (3, 3, 3) untuk mengurangi laju downsampling
model8Adamax2 = Sequential()
model8Adamax2.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model8Adamax2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8Adamax2.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu'))
model8Adamax2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8Adamax2.add(Flatten())
model8Adamax2.add(Dense(units=16, activation='relu'))
model8Adamax2.add(Dropout(0.5))
model8Adamax2.add(Dense(units=3, activation='softmax'))

custom_optimizer = Adamax(learning_rate=0.0010)

model8Adamax2.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

# Reshape data
x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history8Adamax2 = model8Adamax2.fit(x_train, y_train,
                                     epochs=200,
                                     validation_data=(x_val, y_val),
                                     batch_size=32,
                                     shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 8 layer - Adamax - r:0.0100"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adamax
import time

kanal = 12
frekuensi = 42
waktu = 9

# Ubah ukuran pooling menjadi (3, 3, 3) untuk mengurangi laju downsampling
model8Adamax3 = Sequential()
model8Adamax3.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
model8Adamax3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8Adamax3.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu'))
model8Adamax3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model8Adamax3.add(Flatten())
model8Adamax3.add(Dense(units=16, activation='relu'))
model8Adamax3.add(Dropout(0.5))
model8Adamax3.add(Dense(units=3, activation='softmax'))

custom_optimizer = Adamax(learning_rate=0.0100)

model8Adamax3.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

# Reshape data
x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history8Adamax3 = model8Adamax3.fit(x_train, y_train,
                                     epochs=200,
                                     validation_data=(x_val, y_val),
                                     batch_size=32,
                                     shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 16 layer - Adamax - r:0.0001"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adamax
import time

kanal = 12
frekuensi = 42
waktu = 9

model16Adamax1 = Sequential()

model16Adamax1.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1), padding='same'))
model16Adamax1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adamax1.add(Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adamax1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adamax1.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adamax1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adamax1.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adamax1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adamax1.add(Flatten())
model16Adamax1.add(Dense(units=128, activation='relu'))
model16Adamax1.add(Dropout(0.5))
model16Adamax1.add(Dense(units=64, activation='relu'))
model16Adamax1.add(Dropout(0.5))
model16Adamax1.add(Dense(units=32, activation='relu'))
model16Adamax1.add(Dropout(0.5))
model16Adamax1.add(Dense(units=3, activation='softmax'))

custom_optimizer = Adamax(learning_rate=0.0001)

model16Adamax1.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history16Adamax1 = model16Adamax1.fit(x_train, y_train,
                                       epochs=200,
                                       validation_data=(x_val, y_val),
                                       batch_size=32,
                                       shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 16 layer - Adamax - r:0.0010"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adamax
import time

kanal = 12
frekuensi = 42
waktu = 9

model16Adamax2 = Sequential()

model16Adamax2.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1), padding='same'))
model16Adamax2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adamax2.add(Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adamax2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adamax2.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adamax2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adamax2.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adamax2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adamax2.add(Flatten())
model16Adamax2.add(Dense(units=128, activation='relu'))
model16Adamax2.add(Dropout(0.5))
model16Adamax2.add(Dense(units=64, activation='relu'))
model16Adamax2.add(Dropout(0.5))
model16Adamax2.add(Dense(units=32, activation='relu'))
model16Adamax2.add(Dropout(0.5))
model16Adamax2.add(Dense(units=3, activation='softmax'))

custom_optimizer = Adamax(learning_rate=0.0010)

model16Adamax2.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history16Adamax2 = model16Adamax2.fit(x_train, y_train,
                                       epochs=200,
                                       validation_data=(x_val, y_val),
                                       batch_size=32,
                                       shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""cnn 16 layer - Adamax - r:0.0100"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adamax
import time

kanal = 12
frekuensi = 42
waktu = 9

model16Adamax3 = Sequential()

model16Adamax3.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1), padding='same'))
model16Adamax3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adamax3.add(Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adamax3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adamax3.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adamax3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adamax3.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu', padding='same'))
model16Adamax3.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model16Adamax3.add(Flatten())
model16Adamax3.add(Dense(units=128, activation='relu'))
model16Adamax3.add(Dropout(0.5))
model16Adamax3.add(Dense(units=64, activation='relu'))
model16Adamax3.add(Dropout(0.5))
model16Adamax3.add(Dense(units=32, activation='relu'))
model16Adamax3.add(Dropout(0.5))
model16Adamax3.add(Dense(units=3, activation='softmax'))

custom_optimizer = Adamax(learning_rate=0.0100)

model16Adamax3.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history16Adamax3 = model16Adamax3.fit(x_train, y_train,
                                       epochs=200,
                                       validation_data=(x_val, y_val),
                                       batch_size=32,
                                       shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

"""Akurasi"""

#Calculate loss and accuracy
# score4Adamax1 = model4Adamax1.evaluate(x_test,y_test)
# score4Adamax2 = model4Adamax2.evaluate(x_test,y_test)
score4Adamax3 = model4Adamax3.evaluate(x_test,y_test)
# score8Adamax1 = model8Adamax1.evaluate(x_test,y_test)
# score8Adamax2 = model8Adamax2.evaluate(x_test,y_test)
score8Adamax3 = model8Adamax3.evaluate(x_test,y_test)
# score16Adamax1 = model16Adamax1.evaluate(x_test,y_test)
# score16Adamax2 = model16Adamax2.evaluate(x_test,y_test)
score16Adamax3 = model16Adamax3.evaluate(x_test,y_test)

# print('Test Loss Adamax 4 layer lr = 0.0001 : ',score4Adamax1[0])
# print('Test Accuracy Adamax 4 layer lr = 0.0001: ',score4Adamax1[1]*100.0)
# print('========================================')
# print('Test Loss Adamax 4 layer lr = 0.0010 : ',score4Adamax2[0])
# print('Test Accuracy Adamax 4 layer lr = 0.0010: ',score4Adamax2[1]*100.0)
# print('========================================')
print('Test Loss Adamax 4 layer lr = 0.0100 : ',score4Adamax3[0])
print('Test Accuracy Adamax 4 layer lr = 0.0100: ',score4Adamax3[1]*100.0)
print('========================================')

# print('Test Loss Adamax 8 layer lr = 0.0001 : ',score8Adamax1[0])
# print('Test Accuracy Adamax 8 layer lr = 0.0001: ',score8Adamax1[1]*100.0)
# print('========================================')
# print('Test Loss Adamax 8 layer lr = 0.0010 : ',score8Adamax2[0])
# print('Test Accuracy Adamax 8 layer lr = 0.0010: ',score8Adamax2[1]*100.0)
# print('========================================')
print('Test Loss Adamax 8 layer lr = 0.0100 : ',score8Adamax3[0])
print('Test Accuracy Adamax 8 layer lr = 0.0100: ',score8Adamax3[1]*100.0)
print('========================================')

# print('Test Loss Adamax 16 layer lr = 0.0001 : ',score16Adamax1[0])
# print('Test Accuracy Adamax 16 layer lr = 0.0001: ',score16Adamax1[1]*100.0)
# print('========================================')
# print('Test Loss Adamax 16 layer lr = 0.0010 : ',score16Adamax2[0])
# print('Test Accuracy Adamax 16 layer lr = 0.0010: ',score16Adamax2[1]*100.0)
# print('========================================')
print('Test Loss Adamax 16 layer lr = 0.0100 : ',score16Adamax3[0])
print('Test Accuracy Adamax 16 layer lr = 0.0100: ',score16Adamax3[1]*100.0)
print('========================================')

"""#Akurasi Test

Akurasi 1d, 2d,
"""

score_model1d = model1d.evaluate(x_test_reshaped, y_test)
print('Test Loss Model1D: ', score_model1d[0])
print('Test Accuracy Model1D: ', score_model1d[1] * 100.0)

score_model2d = model2d.evaluate(x_test_reshaped_2d, y_test)
print('Test Loss Model2D: ', score_model2d[0])
print('Test Accuracy Model2D: ', score_model2d[1] * 100.0)

scoreAdamax = modelAdamax.evaluate(x_test,y_test)
print('Test Loss Adamax : ',scoreAdamax[0])
print('Test Accuracy : ',scoreAdamax[1]*100.0)
print('========================================')

"""Akurasi Layer Optimasi 4-SGD, 8-SGD, 16-SGD; 4-Adam, 8-Adam, 16-Adam; 4-Adamax, 8-Adamax, 16-Adamax"""

score4SGD3 = model4SGD3.evaluate(x_test,y_test)
score8SGD3 = model8SGD3.evaluate(x_test,y_test)
score16SGD3 = model16SGD3.evaluate(x_test,y_test)

score4Adam3 = model4Adam3.evaluate(x_test,y_test)
score8Adam3 = model8Adam3.evaluate(x_test,y_test)
score16Adam3 = model16Adam3.evaluate(x_test,y_test)

score4Adamax3 = model4Adamax3.evaluate(x_test,y_test)
score8Adamax3 = model8Adamax3.evaluate(x_test,y_test)
score16Adamax3 = model16Adamax3.evaluate(x_test,y_test)


print('Test Loss SGD 4 layer lr = 0.0100 : ',score4SGD3[0])
print('Test Accuracy SGD 4 layer lr = 0.0100: ',score4SGD3[1]*100.0)
print('========================================')
print('Test Loss SGD 8 layer lr = 0.0100 : ',score8SGD3[0])
print('Test Accuracy SGD 8 layer lr = 0.0100: ',score8SGD3[1]*100.0)
print('========================================')
print('Test Loss SGD 16 layer lr = 0.0100 : ',score16SGD3[0])
print('Test Accuracy SGD 16 layer lr = 0.0100: ',score16SGD3[1]*100.0)
print('========================================')

print('Test Loss Adam 4 layer lr = 0.0100 : ',score4Adam3[0])
print('Test Accuracy Adam 4 layer lr = 0.0100: ',score4Adam3[1]*100.0)
print('========================================')
print('Test Loss Adam 8 layer lr = 0.0100 : ',score8Adam3[0])
print('Test Accuracy Adam 8 layer lr = 0.0100: ',score8Adam3[1]*100.0)
print('========================================')
print('Test Loss Adam 16 layer lr = 0.0100 : ',score16Adam3[0])
print('Test Accuracy Adam 16 layer lr = 0.0100: ',score16Adam3[1]*100.0)
print('========================================')

print('Test Loss Adamax 4 layer lr = 0.0100 : ',score4Adamax3[0])
print('Test Accuracy Adamax 4 layer lr = 0.0100: ',score4Adamax3[1]*100.0)
print('========================================')
print('Test Loss Adamax 8 layer lr = 0.0100 : ',score8Adamax3[0])
print('Test Accuracy Adamax 8 layer lr = 0.0100: ',score8Adamax3[1]*100.0)
print('========================================')
print('Test Loss Adamax 16 layer lr = 0.0100 : ',score16Adamax3[0])
print('Test Accuracy Adamax 16 layer lr = 0.0100: ',score16Adamax3[1]*100.0)
print('========================================')

"""Akurasi Parameter Optimasi, 0.0001, 0.0010, 0.0100"""

scoreAdamax = modelAdamax.evaluate(x_test,y_test)
scoreAdamax1 = modelAdamax1.evaluate(x_test,y_test)
scoreAdamax2 = modelAdamax2.evaluate(x_test,y_test)

print('Test Loss Adamax : ',scoreAdamax[0])
print('Test Accuracy : ',scoreAdamax[1]*100.0)
print('========================================')
print('Test Loss Adamax : ',scoreAdamax1[0])
print('Test Accuracy : ',scoreAdamax1[1]*100.0)
print('========================================')
print('Test Loss Adamax : ',scoreAdamax2[0])
print('Test Accuracy : ',scoreAdamax2[1]*100.0)
print('========================================')

"""#Grafik

Grafik 1d, 2d, 3d
"""

#summarize history for accuracy
plt.rcParams["figure.figsize"] = (5,4)

plt.subplot(211)

plt.plot(history_model1d.history['val_categorical_accuracy'])
plt.plot(history_model2d.history['val_categorical_accuracy'])
plt.plot(historyAdamax.history['val_categorical_accuracy'])

plt.title('Methods Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['1D', '2D', '3D'],loc='upper left')
plt.show()

# #sumarize history for losses
plt.subplot(212)
plt.plot(history_model1d.history['val_loss'])
plt.plot(history_model2d.history['val_loss'])
plt.plot(historyAdamax.history['val_loss'])

# plt.plot(history.history['val_loss'])
plt.title('Methods Losses')
plt.ylabel('losses')
plt.xlabel('epoch')
plt.legend(['1D','2D','3D'],loc='upper left')
plt.show()

"""Grafik Layer Optimasi 4-SGD, 8-SGD, 16-SGD; 4-Adam, 8-Adam, 16-Adam; 4-Adamax, 8-Adamax, 16-Adamax"""

#summarize history for accuracy
plt.rcParams["figure.figsize"] = (7,4)

plt.subplot(211)
plt.plot(history4SGD3.history['val_categorical_accuracy'])
plt.plot(history8SGD3.history['val_categorical_accuracy'])
plt.plot(history4SGD3.history['val_categorical_accuracy'])

# plt.plot(history4Adam3.history['val_categorical_accuracy'])
# plt.plot(history8Adam3.history['val_categorical_accuracy'])
# plt.plot(history4Adam3.history['val_categorical_accuracy'])

plt.plot(historyAdamax.history['val_categorical_accuracy'])
plt.plot(historyAdamax1.history['val_categorical_accuracy'])
plt.plot(historyAdamax2.history['val_categorical_accuracy'])

plt.title('Layers Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['4-SGD', '8-SGD', '16-SGD', '4-Adamax', '8-Adamax', '16-Adamax'],loc='upper center', bbox_to_anchor=(0.5, -0.3), fancybox=True, shadow=True, ncol=2)
# plt.legend(['4-SGD', '8-SGD', '16-SGD', '4-Adam', '8-Adam', '16-Adam', '4-Adamax', '8-Adamax', '16-Adamax'],loc='upper center', bbox_to_anchor=(0.5, -0.3), fancybox=True, shadow=True, ncol=3)
plt.show()

# #sumarize history for losses
plt.subplot(212)
plt.plot(history4SGD3.history['val_loss'])
plt.plot(history8SGD3.history['val_loss'])
plt.plot(history4SGD3.history['val_loss'])

# plt.plot(history4Adam3.history['val_loss'])
# plt.plot(history8Adam3.history['val_loss'])
# plt.plot(history4Adam3.history['val_loss'])

plt.plot(historyAdamax.history['val_loss'])
plt.plot(historyAdamax1.history['val_loss'])
plt.plot(historyAdamax2.history['val_loss'])

# plt.plot(history.history['val_loss'])
plt.title('Layers Losses')
plt.ylabel('losses')
plt.xlabel('epoch')
# plt.legend(['4-SGD', '8-SGD', '16-SGD', '4-Adam', '8-Adam', '16-Adam', '4-Adamax', '8-Adamax', '16-Adamax'],loc='upper center', bbox_to_anchor=(0.5, -0.3), fancybox=True, shadow=True, ncol=3)
plt.legend(['4-SGD', '8-SGD', '16-SGD', '4-Adamax', '8-Adamax', '16-Adamax'],loc='upper center', bbox_to_anchor=(0.5, -0.3), fancybox=True, shadow=True, ncol=2)
plt.show()

"""Grafik Parameter Optimasi, 0.0001, 0.0010, 0.0100"""

#summarize history for accuracy
plt.rcParams["figure.figsize"] = (5,4)

plt.subplot(211)
plt.plot(historyAdamax.history['val_categorical_accuracy'])
plt.plot(historyAdamax1.history['val_categorical_accuracy'])
plt.plot(historyAdamax2.history['val_categorical_accuracy'])

plt.title('Parameter Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['0.0001', '0.0010', '0.0100'],loc='upper left')
plt.show()

# #sumarize history for losses
plt.subplot(212)
plt.plot(historyAdamax.history['val_loss'])
plt.plot(historyAdamax1.history['val_loss'])
plt.plot(historyAdamax2.history['val_loss'])

# plt.plot(history.history['val_loss'])
plt.title('Parameter Losses')
plt.ylabel('losses')
plt.xlabel('epoch')
plt.legend(['0.0001', '0.0010', '0.0100'],loc='upper left')
plt.show()

"""#ex"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adamax
import time

#Learning Adamax
kanal = 12
frekuensi = 42
waktu = 9

modelAdamax = Sequential()
modelAdamax.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
modelAdamax.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
modelAdamax.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu'))
modelAdamax.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
modelAdamax.add(Flatten())
modelAdamax.add(Dense(units=128, activation='relu'))
modelAdamax.add(Dropout(0.5))
modelAdamax.add(Dense(units=3, activation='softmax'))
modelAdamax.summary()

custom_optimizer = Adamax(learning_rate=0.0001)

modelAdamax.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
historyAdamax = modelAdamax.fit(x_train, y_train,
                                       epochs=200,
                                       validation_data=(x_val, y_val),
                                       batch_size=32,
                                       shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adamax
import time

# Learning Adamax
kanal = 12
frekuensi = 42
waktu = 9

modelAdamax1 = Sequential()
modelAdamax1.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
modelAdamax1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
modelAdamax1.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu'))
modelAdamax1.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
modelAdamax1.add(Flatten())
modelAdamax1.add(Dense(units=128, activation='relu'))
modelAdamax1.add(Dropout(0.5))
modelAdamax1.add(Dense(units=3, activation='softmax'))
modelAdamax1.summary()

custom_optimizer = Adamax(learning_rate=0.0010)

modelAdamax1.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
historyAdamax1 = modelAdamax1.fit(x_train, y_train,
                                  epochs=200,
                                  validation_data=(x_val, y_val),
                                  batch_size=32,
                                  shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adamax
import time

# Learning Adamax
kanal = 12
frekuensi = 42
waktu = 9

modelAdamax2 = Sequential()
modelAdamax2.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal, frekuensi, waktu, 1)))
modelAdamax2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
modelAdamax2.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu'))
modelAdamax2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
modelAdamax2.add(Flatten())
modelAdamax2.add(Dense(units=128, activation='relu'))
modelAdamax2.add(Dropout(0.5))
modelAdamax2.add(Dense(units=3, activation='softmax'))
modelAdamax2.summary()

custom_optimizer = Adamax(learning_rate=0.0100)

modelAdamax2.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['categorical_accuracy'])

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
historyAdamax2 = modelAdamax2.fit(x_train, y_train,
                                  epochs=200,
                                  validation_data=(x_val, y_val),
                                  batch_size=32,
                                  shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning:", ((time.time() - start_time) / 60))

scoreAdamax = modelAdamax.evaluate(x_test,y_test)
scoreAdamax1 = modelAdamax1.evaluate(x_test,y_test)
scoreAdamax2 = modelAdamax2.evaluate(x_test,y_test)

print('Test Loss Adamax : ',scoreAdamax[0])
print('Test Accuracy : ',scoreAdamax[1]*100.0)
print('========================================')
print('Test Loss Adamax : ',scoreAdamax1[0])
print('Test Accuracy : ',scoreAdamax1[1]*100.0)
print('========================================')
print('Test Loss Adamax : ',scoreAdamax2[0])
print('Test Accuracy : ',scoreAdamax2[1]*100.0)
print('========================================')

#Learning Adamax
kanal = 12
frekuensi = 42
waktu = 9

model = Sequential()
model.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu', input_shape=(kanal,frekuensi, waktu, 1)))
model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu'))
model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(units=128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=3, activation='softmax'))
# modelAdam.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

model.compile(loss='categorical_crossentropy', optimizer='Adamax', metrics=['categorical_accuracy'])

import time

x_train = x_train.reshape(174, kanal, frekuensi, waktu, 1)
x_val = x_val.reshape(44, kanal, frekuensi, waktu, 1)
x_test = x_test.reshape(55, kanal, frekuensi, waktu, 1)

print("Training model dimulai")
start_time = time.time()
history = model.fit(x_train,y_train,
                    epochs=200,
                    validation_data=(x_val,y_val),
                    batch_size=32,
                    shuffle=True)

print("\nTraining model selesai")
print("Lama waktu learning : ",((time.time() - start_time)/60))

score = model.evaluate(x_test,y_test)

print('Test Loss SGD : ',score[0])
print('Test Accuracy SGD : ',score[1]*100.0)
print('========================================')